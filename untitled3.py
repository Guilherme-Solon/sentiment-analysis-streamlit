# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mY3MpBHDs3XA6LanLZmAMy9J1mVX_ec7
"""

!pip install -q streamlit pyngrok nltk matplotlib seaborn

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
# # Parte 2: Código do app com melhorias e pré-processamento
# 
# %%writefile app.py
# import pandas as pd
# import streamlit as st
# import matplotlib.pyplot as plt
# import seaborn as sns
# import nltk
# import string
# from sklearn.feature_extraction.text import CountVectorizer
# from sklearn.model_selection import train_test_split
# from sklearn.naive_bayes import MultinomialNB
# from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer
# 
# # Baixar recursos nltk
# nltk.download('stopwords')
# nltk.download('wordnet')
# 
# def preprocess_text(text):
#     lemmatizer = WordNetLemmatizer()
#     stop_words = set(stopwords.words('english'))
#     text = text.lower()
#     text = ''.join([ch for ch in text if ch not in string.punctuation])
#     tokens = text.split()
#     tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
#     return ' '.join(tokens)
# 
# @st.cache_data
# def load_data():
#     df = pd.read_csv("sentimentdataset.csv")
#     df = df[['Text', 'Sentiment']].dropna()
#     df['Clean_Text'] = df['Text'].apply(preprocess_text)
#     return df
# 
# def plot_class_distribution(df):
#     fig, ax = plt.subplots()
#     sns.countplot(x='Sentiment', data=df, ax=ax)
#     ax.set_title("Distribuição de classes")
#     st.pyplot(fig)
# 
# def train_model(df):
#     vectorizer = CountVectorizer()
#     X = vectorizer.fit_transform(df['Clean_Text'])
#     y = df['Sentiment']
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
#     model = MultinomialNB()
#     model.fit(X_train, y_train)
#     preds = model.predict(X_test)
#     acc = accuracy_score(y_test, preds)
#     cm = confusion_matrix(y_test, preds)
#     cr = classification_report(y_test, preds, output_dict=True)
#     return model, vectorizer, acc, cm, cr
# 
# def plot_confusion_matrix(cm):
#     fig, ax = plt.subplots()
#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
#     ax.set_xlabel("Predito")
#     ax.set_ylabel("Real")
#     ax.set_title("Matriz de Confusão")
#     st.pyplot(fig)
# 
# def main():
#     st.title("Analisador de Sentimentos Melhorado")
#     st.write("Carregando dados e preparando...")
# 
#     df = load_data()
# 
#     st.subheader("Dados originais")
#     st.write(df.head())
# 
#     st.subheader("Distribuição de classes")
#     plot_class_distribution(df)
# 
#     st.write("Treinando modelo...")
#     model, vectorizer, acc, cm, cr = train_model(df)
# 
#     st.write(f"Acurácia do modelo: *{acc:.2f}*")
# 
#     st.subheader("Matriz de Confusão")
#     plot_confusion_matrix(cm)
# 
#     st.subheader("Relatório de Classificação")
#     st.json(cr)
# 
#     frase = st.text_input("Digite um texto para analisar o sentimento:")
#     if frase:
#         frase_limpa = preprocess_text(frase)
#         entrada = vectorizer.transform([frase_limpa])
#         pred = model.predict(entrada)[0]
#         st.write(f"Sentimento previsto: *{pred}*")
# 
# if __name__ == "__main__":
#     main()

!nohup streamlit run app.py --server.port 8501 &

# Adiciona o authtoken do ngrok
!ngrok config add-authtoken 2yhUsc2S4Kiy2yn3IFFAvHpfbFM_6xEJQWbWDwe33A5bgUZfP

from pyngrok import ngrok

public_url = ngrok.connect(8501)
print(f"Acesse o app em: {public_url}")